attention_mask=
pdrop forward=torch.Size([1, 925])
before torch.Size([1, 925])
4d_casual_mask   2D--->4D
last torch.Size([1, 1, 925, 925])


labels ([1,925])


input embeds=([1,925,4096])




features= torch.Size([1, 925, 4096])
